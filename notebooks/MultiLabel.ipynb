{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.setrecursionlimit(1000000)\n",
    "\n",
    "from spectral import get_rgb, ndvi\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from skimage.io import imread\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.callbacks import CSVLogger, ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "import paths\n",
    "\n",
    "from rainforest.data import get_data, labels\n",
    "from rainforest.preprocess import preprocess\n",
    "from rainforest.models.resnet import ResNet50\n",
    "from rainforest.models.densenet import DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = get_data(train=True)\n",
    "val_data = get_data(train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=12\n",
    "image_type = 'both' # 'jpg', 'tif' or 'both'\n",
    "n_channels = 3 if image_type == 'jpg' else 5 if image_type == 'tif' else 8\n",
    "input_shape = (n_channels, 64, 64)\n",
    "input_shape = image_size if K.image_dim_ordering() == 'th' else tuple(reversed(input_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_image1(img):\n",
    "    img_rgb = get_rgb(img, [2, 1, 0])  # R-G-B\n",
    "    rescaleimg = np.reshape(img_rgb, (-1, 1))\n",
    "    scaler = MinMaxScaler(feature_range=(0, 255))\n",
    "    rescaleimg = scaler.fit_transform(rescaleimg)  # .astype(np.float32)\n",
    "    img_scaled = (np.reshape(rescaleimg, img_rgb.shape)) / 255.\n",
    "    img_nir = get_rgb(img, [3, 2, 1])  # NIR-R-G\n",
    "    img_nir_red = (img_nir[:, :, 0] - img_nir[:, :, 1]) / (img_nir[:, :, 0] + img_nir[:, :, 1] + np.finfo(float).eps)  # (NIR - RED) / (NIR + RED)\n",
    "    img_nir_red = np.expand_dims(np.clip(img_nir_red, -1, 1), axis=2)\n",
    "    img_nir_green = (img_nir[:, :, 2] - img_nir[:, :, 0]) / (img_nir[:, :, 2] + img_nir[:, :, 0] + np.finfo(float).eps)  # (GREEN - NIR) / (GREEN + NIR)\n",
    "    img_nir_green = np.expand_dims(np.clip(img_nir_green, -1, 1), axis=2)\n",
    "\n",
    "    return np.dstack((img_scaled, img_nir_red, img_nir_green))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_generator(data_df, batch_size=32, target_size=(3, 256, 256), shuffle=True, augmentation=True,\n",
    "                   mode='train', image_type='jpg'):\n",
    "    n = len(data_df)\n",
    "    use_tif = image_type in ['tif', 'both']\n",
    "    use_jpg = image_type in ['jpg', 'both']\n",
    "    cv2_image_shape = tuple(reversed(target_size[1:] if K.image_dim_ordering() == 'th' else target_size[:2]))\n",
    "    while True:\n",
    "        # Maybe shuffle\n",
    "        data = data_df.sample(frac=1) if shuffle else data_df\n",
    "        data = data.append(data, ignore_index=True)\n",
    "        i = 0\n",
    "        while i < n:\n",
    "            X_batch = np.zeros((batch_size,)+target_size , dtype=np.float32)\n",
    "            y_batch = np.zeros((batch_size, 17), dtype=np.uint8)\n",
    "            \n",
    "            for j in range(batch_size):\n",
    "                img = data.iloc[i]\n",
    "                images = []\n",
    "                if use_tif:\n",
    "                    tif_path = os.path.join(paths.DATA_FOLDER, mode+'-tif-v2', img.image_name+'.tif')\n",
    "                    tif_image = imread(tif_path)\n",
    "                    tif_image = preprocess_image1(tif_image)\n",
    "                    images.append(tif_image)\n",
    "                    \n",
    "                if use_jpg:\n",
    "                    jpg_path = os.path.join(paths.DATA_FOLDER, mode+'-jpg', img.image_name+'.jpg')\n",
    "                    jpg_image = cv2.imread(jpg_path, cv2.IMREAD_COLOR)[:, :, :3]/255.\n",
    "                    images.append(jpg_image)\n",
    "                \n",
    "                image = np.dstack(images)\n",
    "                image = preprocess(image, target_size=cv2_image_shape, augmentation=augmentation,\n",
    "                                   hflip=True, vflip=True, shift_x=3, shift_y=3, rot_range=5,\n",
    "                                   dim_ordering=K.image_dim_ordering())\n",
    "                X_batch[j] = image\n",
    "                y_batch[j] = img[1:].values\n",
    "                i += 1\n",
    "\n",
    "            yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fb_score(beta=1, smooth=1e-6, threshold=0.2):\n",
    "    axis = 1 if K.image_dim_ordering()  == 'th' else -1\n",
    "    def fscore(y_true, y_pred):\n",
    "        y_pred = y_pred > threshold\n",
    "        recall = (K.sum(y_true * y_pred, axis=axis) + smooth) / (K.sum(y_true, axis=axis) + smooth)\n",
    "        precision = (K.sum(y_true * y_pred, axis=axis) + smooth) / (K.sum(y_pred, axis=axis) + smooth)\n",
    "        return K.mean( ((1+beta**2) * (precision*recall)+smooth) / (beta**2*precision+recall+smooth) )\n",
    "    \n",
    "    fscore.__name__ = 'F%d_score' % beta\n",
    "    \n",
    "    return fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def resnet_like():\n",
    "    model = ResNet50(input_shape=input_shape, classes=17, classification='sigmoid', layer1_filters=32)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vgg_like():\n",
    "    model = Sequential([\n",
    "        Conv2D(32, 3, activation='relu', kernel_initializer='he_normal', input_shape=input_shape\n",
    "        BatchNormalization(axis=1),\n",
    "        Conv2D(32, 3, activation='relu', kernel_initializer='he_normal'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPool2D(),\n",
    "\n",
    "        Conv2D(64, 3, activation='relu', kernel_initializer='he_normal'),\n",
    "        BatchNormalization(axis=1),\n",
    "        Conv2D(64, 3, activation='relu', kernel_initializer='he_normal'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPool2D(),\n",
    "\n",
    "        Conv2D(128, 3, activation='relu', kernel_initializer='he_normal'),\n",
    "        BatchNormalization(axis=1),\n",
    "        Conv2D(128, 3, activation='relu', kernel_initializer='he_normal'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPool2D(),\n",
    "\n",
    "        Flatten(),\n",
    "        Dense(1024, activation='relu', kernel_initializer='he_normal'),\n",
    "        BatchNormalization(),\n",
    "        Dense(17, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DenseNet(input_shape=input_shape, depth=121, bottleneck=True, reduction=0.5, weight_decay=0, weights=None, classes=17, activation='sigmoid')\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', fb_score(beta=2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_gen = data_generator(train_data, batch_size=batch_size, target_size=input_size, shuffle=True, augmentation=True,\n",
    "                           mode='train', image_type='both')\n",
    "val_gen = data_generator(val_data, batch_size=batch_size, target_size=input_size, shuffle=False, augmentation=False,\n",
    "                        mode='train', image_type='both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_logger = CSVLogger('log.csv')\n",
    "lr_plateau = ReduceLROnPlateau(monitor='val_loss', patience=3, verbose=1, factor=0.25)\n",
    "checkpoint = ModelCheckpoint(filepath='E:/Models/brainforest/densenet_64x64_tifjpg.hdf5', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1189/2361 [==============>...............] - ETA: 4078s - loss: 0.1801 - acc: 0.9271 - F2_score: 0.8282"
     ]
    }
   ],
   "source": [
    "train_steps = len(train_data) // batch_size\n",
    "val_steps = len(val_data) // batch_size\n",
    "model.fit_generator(train_gen, train_steps, epochs=50, callbacks=[csv_logger, lr_plateau, checkpoint],\n",
    "                    validation_data=val_gen, validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('E:/Models/brainforest/densenet_64x64_tifjpg.hdf5', custom_objects={'F2_score': fb_score(beta=2)})\n",
    "\n",
    "def strip_labels(gen):\n",
    "    while True:\n",
    "        imgs, _ = next(gen)\n",
    "        yield imgs\n",
    "\n",
    "val_steps = int(np.ceil(len(val_data) // batch_size)) + 1\n",
    "val_gen = strip_labels(data_generator(val_data, batch_size=batch_size, target_size=input_size, shuffle=False, mode='train', image_type='both'))\n",
    "preds = model.predict_generator(val_gen, val_steps, verbose=1)\n",
    "preds = preds[:len(val_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for threshold in np.arange(0.1, 0.3, 0.02):\n",
    "    y_true = val_data.iloc[:, 1:].values\n",
    "    y_pred =  preds > threshold\n",
    "    print threshold, 'f2 score:', fbeta_score(y_true, y_pred, 2, average='samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_files = glob.glob(os.path.join(paths.DATA_FOLDER, 'test-jpg', '*.jpg'))\n",
    "test_files = [os.path.basename(os.path.splitext(f)[0]) for f in test_files]\n",
    "test_data = pd.DataFrame(test_files, columns=['image_name'])\n",
    "test_data['bogus_label'] = np.zeros(len(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_steps = int(np.ceil(len(test_data) // batch_size)) + 1\n",
    "test_gen = strip_labels(data_generator(test_data, batch_size=batch_size, target_size=input_size, shuffle=False, subfolder='test-jpg'))\n",
    "preds = model.predict_generator(test_gen, test_steps)\n",
    "preds = preds[:len(test_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tpreds = preds > 0.16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('submission2.csv', 'w') as file:\n",
    "    file.write('image_name,tags\\n')\n",
    "    for img, pred in zip(test_files, tpreds):\n",
    "        indices = np.flatnonzero(pred)\n",
    "        tags = ' '.join([labels[i] for i in indices])\n",
    "        file.write('%s,%s\\n' % (img, tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(preds, 'multilabel2405_128x128.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = (joblib.load('multilabel2405_128x128.pkl') + joblib.load('multilabel2405.pkl')) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tpreds = preds > 0.16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('submission3.csv', 'w') as file:\n",
    "    file.write('image_name,tags\\n')\n",
    "    for img, pred in zip(test_files, tpreds):\n",
    "        indices = np.flatnonzero(pred)\n",
    "        tags = ' '.join([labels[i] for i in indices])\n",
    "        file.write('%s,%s\\n' % (img, tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
