{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primary vs Primary + Blooming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook all the images with the label primary (but not blooming) will be put against images with the label primary and blooming. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using cuDNN version 5110 on context None\n",
      "Mapped name None to device cuda0: GeForce GTX 1050 Ti (0000:09:00.0)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import Conv2D, Dense, Flatten, Dropout, MaxPooling2D, BatchNormalization\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import CSVLogger, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "import paths\n",
    "sys.path.insert(0, '../rainforest')\n",
    "import data\n",
    "import data_generators_leo as dgl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading training data and corresponding labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images = ['image_name', 'primary', 'blooming']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26284, 1)\n",
      "(11229, 1)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "size_img = 92\n",
    "imgs = (size_img,size_img)\n",
    "#train_csv = data.get_class_data(train=True, label='primary')\n",
    "\n",
    "train_csv = np.asarray(data.get_class_data(train=True, label='primary')) #everything in onehot\n",
    "val_csv =  np.asarray(data.get_class_data(train=False, label='primary')) #everything in onehot\n",
    "\n",
    "train_data = []\n",
    "for i in train_csv:\n",
    "    if i[1] == 1:\n",
    "        appending = [i[0], int(i[1]), int(i[2])]\n",
    "        train_data.append(appending)\n",
    "train_csv = np.asarray(train_data)\n",
    "\n",
    "val_data = []\n",
    "for i in val_csv:\n",
    "    if i[1] == 1:\n",
    "        appending = [i[0], int(i[1]), int(i[2])]\n",
    "        val_data.append(appending)\n",
    "val_csv = np.asarray(val_data)\n",
    "\n",
    "\n",
    "train_data = train_csv[:,0] #image names training\n",
    "val_data = val_csv[:,0] #image names validation\n",
    "\n",
    "\n",
    "train_labels = train_csv[:,2:] \n",
    "val_labels = val_csv[:,2:]\n",
    "\n",
    "print train_labels.shape\n",
    "print val_labels.shape\n",
    "\n",
    "#train_labels = train_csv[:,11:12] #primary\n",
    "#val_labels = val_csv[:,2:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = len(train_labels)\n",
    "dimlabel = np.zeros((dim, 2))\n",
    "for i, j in zip(train_labels, dimlabel):\n",
    "    j[int(i[0])] = 1\n",
    "train_labels = dimlabel\n",
    "\n",
    "dim = len(val_labels)\n",
    "vallabel = np.zeros((dim, 2))\n",
    "for i, j in zip(val_labels, vallabel):\n",
    "    j[int(i[0])] = 1\n",
    "val_labels = vallabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26284, 2)\n",
      "(11229, 2)\n",
      "(11229,)\n",
      "(26284,)\n",
      "[ 1.  0.]\n",
      "[ 1.  0.]\n",
      "train_8337\n",
      "train_19035\n"
     ]
    }
   ],
   "source": [
    "print train_labels.shape\n",
    "print val_labels.shape\n",
    "print val_data.shape\n",
    "print train_data.shape\n",
    "\n",
    "print train_labels[1]\n",
    "print val_labels[1]\n",
    "print train_data[1]\n",
    "print val_data[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_gen = dgl.get_data(train_data, '../data/train-jpg', train_labels, batch_size=batch_size, img_size=imgs, balance_batches=True, augmentation=True, hflip=True, vflip=True, shift_x=5, shift_y=5, rot_range=10)\n",
    "val_gen = dgl.get_data(val_data, '../data/train-jpg', val_labels, batch_size=batch_size, img_size=imgs, balance_batches=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 16, 90, 90)        448       \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 16, 88, 88)        2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 16, 44, 44)        0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 16, 44, 44)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 32, 42, 42)        4640      \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 32, 40, 40)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 32, 20, 20)        0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 32, 20, 20)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 64, 18, 18)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 64, 16, 16)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 64, 8, 8)          0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 64, 8, 8)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               1048832   \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 1,121,426.0\n",
      "Trainable params: 1,121,426.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(3, 92, 92)))\n",
    "model.add(Conv2D(16, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256))\n",
    "model.add(Dropout(0.50))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='Adam', loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "821\n",
      "350\n",
      "Epoch 1/50\n",
      "820/821 [============================>.] - ETA: 0s - loss: 0.6826 - acc: 0.5591Epoch 00000: val_loss improved from inf to 0.68355, saving model to /home/pieter/projects/MLIP-Brainforest/bloomingtest/OWN/model.00-0.68.hdf5\n",
      "821/821 [==============================] - 68s - loss: 0.6825 - acc: 0.5592 - val_loss: 0.6836 - val_acc: 0.5723\n",
      "Epoch 2/50\n",
      "820/821 [============================>.] - ETA: 0s - loss: 0.6645 - acc: 0.5792Epoch 00001: val_loss did not improve\n",
      "821/821 [==============================] - 67s - loss: 0.6644 - acc: 0.5792 - val_loss: 0.6876 - val_acc: 0.5595\n",
      "Epoch 3/50\n",
      "820/821 [============================>.] - ETA: 0s - loss: 0.6563 - acc: 0.5894\n",
      "Epoch 00002: reducing learning rate to 0.000500000023749.\n",
      "Epoch 00002: val_loss did not improve\n",
      "821/821 [==============================] - 67s - loss: 0.6563 - acc: 0.5894 - val_loss: 0.6871 - val_acc: 0.5810\n",
      "Epoch 4/50\n",
      "820/821 [============================>.] - ETA: 0s - loss: 0.6304 - acc: 0.6112\n",
      "Epoch 00003: reducing learning rate to 0.000250000011874.\n",
      "Epoch 00003: val_loss did not improve\n",
      "821/821 [==============================] - 67s - loss: 0.6303 - acc: 0.6112 - val_loss: 0.6954 - val_acc: 0.5658\n",
      "Epoch 5/50\n",
      "820/821 [============================>.] - ETA: 0s - loss: 0.6113 - acc: 0.6306\n",
      "Epoch 00004: reducing learning rate to 0.000125000005937.\n",
      "Epoch 00004: val_loss did not improve\n",
      "821/821 [==============================] - 67s - loss: 0.6113 - acc: 0.6306 - val_loss: 0.7102 - val_acc: 0.5467\n",
      "Epoch 6/50\n",
      "820/821 [============================>.] - ETA: 0s - loss: 0.5956 - acc: 0.6450\n",
      "Epoch 00005: reducing learning rate to 6.25000029686e-05.\n",
      "Epoch 00005: val_loss did not improve\n",
      "821/821 [==============================] - 67s - loss: 0.5955 - acc: 0.6451 - val_loss: 0.7099 - val_acc: 0.5529\n",
      "Epoch 7/50\n",
      "820/821 [============================>.] - ETA: 0s - loss: 0.5859 - acc: 0.6513\n",
      "Epoch 00006: reducing learning rate to 3.12500014843e-05.\n",
      "Epoch 00006: val_loss did not improve\n",
      "821/821 [==============================] - 67s - loss: 0.5860 - acc: 0.6512 - val_loss: 0.7171 - val_acc: 0.5618\n",
      "Epoch 8/50\n",
      "820/821 [============================>.] - ETA: 0s - loss: 0.5816 - acc: 0.6561\n",
      "Epoch 00007: reducing learning rate to 1.56250007421e-05.\n",
      "Epoch 00007: val_loss did not improve\n",
      "821/821 [==============================] - 67s - loss: 0.5817 - acc: 0.6559 - val_loss: 0.7269 - val_acc: 0.5572\n",
      "Epoch 9/50\n",
      "820/821 [============================>.] - ETA: 0s - loss: 0.5788 - acc: 0.6578\n",
      "Epoch 00008: reducing learning rate to 7.81250037107e-06.\n",
      "Epoch 00008: val_loss did not improve\n",
      "821/821 [==============================] - 67s - loss: 0.5788 - acc: 0.6577 - val_loss: 0.7281 - val_acc: 0.5549\n",
      "Epoch 10/50\n",
      "820/821 [============================>.] - ETA: 0s - loss: 0.5795 - acc: 0.6590\n",
      "Epoch 00009: reducing learning rate to 3.90625018554e-06.\n",
      "Epoch 00009: val_loss did not improve\n",
      "821/821 [==============================] - 67s - loss: 0.5795 - acc: 0.6591 - val_loss: 0.7290 - val_acc: 0.5517\n",
      "Epoch 11/50\n",
      "820/821 [============================>.] - ETA: 0s - loss: 0.5778 - acc: 0.6592\n",
      "Epoch 00010: reducing learning rate to 1.95312509277e-06.\n",
      "Epoch 00010: val_loss did not improve\n",
      "821/821 [==============================] - 67s - loss: 0.5778 - acc: 0.6591 - val_loss: 0.7295 - val_acc: 0.5500\n",
      "Epoch 12/50\n",
      "820/821 [============================>.] - ETA: 0s - loss: 0.5778 - acc: 0.6607\n",
      "Epoch 00011: reducing learning rate to 9.76562546384e-07.\n",
      "Epoch 00011: val_loss did not improve\n",
      "821/821 [==============================] - 67s - loss: 0.5779 - acc: 0.6607 - val_loss: 0.7301 - val_acc: 0.5497\n",
      "Epoch 13/50\n",
      "820/821 [============================>.] - ETA: 0s - loss: 0.5771 - acc: 0.6588\n",
      "Epoch 00012: reducing learning rate to 4.88281273192e-07.\n",
      "Epoch 00012: val_loss did not improve\n",
      "821/821 [==============================] - 67s - loss: 0.5772 - acc: 0.6586 - val_loss: 0.7297 - val_acc: 0.5510\n",
      "Epoch 14/50\n",
      "820/821 [============================>.] - ETA: 0s - loss: 0.5783 - acc: 0.6572\n",
      "Epoch 00013: reducing learning rate to 2.44140636596e-07.\n",
      "Epoch 00013: val_loss did not improve\n",
      "821/821 [==============================] - 67s - loss: 0.5782 - acc: 0.6574 - val_loss: 0.7295 - val_acc: 0.5519\n",
      "Epoch 15/50\n",
      "820/821 [============================>.] - ETA: 0s - loss: 0.5772 - acc: 0.6602\n",
      "Epoch 00014: reducing learning rate to 1.22070318298e-07.\n",
      "Epoch 00014: val_loss did not improve\n",
      "821/821 [==============================] - 67s - loss: 0.5772 - acc: 0.6602 - val_loss: 0.7302 - val_acc: 0.5507\n",
      "Epoch 16/50\n",
      "820/821 [============================>.] - ETA: 0s - loss: 0.5765 - acc: 0.6596\n",
      "Epoch 00015: reducing learning rate to 6.1035159149e-08.\n",
      "Epoch 00015: val_loss did not improve\n",
      "821/821 [==============================] - 67s - loss: 0.5766 - acc: 0.6596 - val_loss: 0.7302 - val_acc: 0.5510\n",
      "Epoch 17/50\n",
      "820/821 [============================>.] - ETA: 0s - loss: 0.5743 - acc: 0.6641\n",
      "Epoch 00016: reducing learning rate to 3.05175795745e-08.\n",
      "Epoch 00016: val_loss did not improve\n",
      "821/821 [==============================] - 67s - loss: 0.5743 - acc: 0.6641 - val_loss: 0.7294 - val_acc: 0.5513\n",
      "Epoch 18/50\n",
      "820/821 [============================>.] - ETA: 0s - loss: 0.5768 - acc: 0.6607\n",
      "Epoch 00017: reducing learning rate to 1.52587897873e-08.\n",
      "Epoch 00017: val_loss did not improve\n",
      "821/821 [==============================] - 67s - loss: 0.5768 - acc: 0.6607 - val_loss: 0.7304 - val_acc: 0.5507\n",
      "Epoch 19/50\n",
      "820/821 [============================>.] - ETA: 0s - loss: 0.5774 - acc: 0.6588\n",
      "Epoch 00018: reducing learning rate to 7.62939489363e-09.\n",
      "Epoch 00018: val_loss did not improve\n",
      "821/821 [==============================] - 67s - loss: 0.5773 - acc: 0.6589 - val_loss: 0.7293 - val_acc: 0.5508\n",
      "Epoch 20/50\n",
      "820/821 [============================>.] - ETA: 0s - loss: 0.5788 - acc: 0.6585\n",
      "Epoch 00019: reducing learning rate to 3.81469744681e-09.\n",
      "Epoch 00019: val_loss did not improve\n",
      "821/821 [==============================] - 68s - loss: 0.5789 - acc: 0.6586 - val_loss: 0.7298 - val_acc: 0.5514\n",
      "Epoch 21/50\n",
      "820/821 [============================>.] - ETA: 0s - loss: 0.5768 - acc: 0.6591\n",
      "Epoch 00020: reducing learning rate to 1.90734872341e-09.\n",
      "Epoch 00020: val_loss did not improve\n",
      "821/821 [==============================] - 68s - loss: 0.5769 - acc: 0.6591 - val_loss: 0.7295 - val_acc: 0.5515\n",
      "Epoch 22/50\n",
      "820/821 [============================>.] - ETA: 0s - loss: 0.5786 - acc: 0.6566\n",
      "Epoch 00021: reducing learning rate to 9.53674361703e-10.\n",
      "Epoch 00021: val_loss did not improve\n",
      "821/821 [==============================] - 67s - loss: 0.5785 - acc: 0.6568 - val_loss: 0.7304 - val_acc: 0.5505\n",
      "Epoch 23/50\n",
      "820/821 [============================>.] - ETA: 0s - loss: 0.5784 - acc: 0.6559\n",
      "Epoch 00022: reducing learning rate to 4.76837180852e-10.\n",
      "Epoch 00022: val_loss did not improve\n",
      "821/821 [==============================] - 67s - loss: 0.5783 - acc: 0.6562 - val_loss: 0.7302 - val_acc: 0.5508\n",
      "Epoch 24/50\n",
      "820/821 [============================>.] - ETA: 0s - loss: 0.5785 - acc: 0.6596- ETA: 1s - loss\n",
      "Epoch 00023: reducing learning rate to 2.38418590426e-10.\n",
      "Epoch 00023: val_loss did not improve\n",
      "821/821 [==============================] - 67s - loss: 0.5785 - acc: 0.6595 - val_loss: 0.7302 - val_acc: 0.5509\n",
      "Epoch 25/50\n",
      "820/821 [============================>.] - ETA: 0s - loss: 0.5788 - acc: 0.6601\n",
      "Epoch 00024: reducing learning rate to 1.19209295213e-10.\n",
      "Epoch 00024: val_loss did not improve\n",
      "821/821 [==============================] - 67s - loss: 0.5789 - acc: 0.6600 - val_loss: 0.7290 - val_acc: 0.5517\n",
      "Epoch 26/50\n",
      "820/821 [============================>.] - ETA: 0s - loss: 0.5770 - acc: 0.6604\n",
      "Epoch 00025: reducing learning rate to 5.96046476065e-11.\n",
      "Epoch 00025: val_loss did not improve\n",
      "821/821 [==============================] - 68s - loss: 0.5771 - acc: 0.6602 - val_loss: 0.7306 - val_acc: 0.5506\n",
      "Epoch 27/50\n",
      "820/821 [============================>.] - ETA: 0s - loss: 0.5755 - acc: 0.6620\n",
      "Epoch 00026: reducing learning rate to 2.98023238032e-11.\n",
      "Epoch 00026: val_loss did not improve\n",
      "821/821 [==============================] - 69s - loss: 0.5755 - acc: 0.6620 - val_loss: 0.7294 - val_acc: 0.5513\n",
      "Epoch 28/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420/821 [==============>...............] - ETA: 24s - loss: 0.5795 - acc: 0.6574"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-656eebac1d7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcsv_logger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_plateau\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     validation_data=val_gen, validation_steps=steps_val)\n\u001b[0m",
      "\u001b[0;32m/home/pieter/anaconda2/envs/mlippython2/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pieter/anaconda2/envs/mlippython2/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1095\u001b[0m                                         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m                                         \u001b[0mpickle_safe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_safe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1097\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pieter/anaconda2/envs/mlippython2/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pieter/anaconda2/envs/mlippython2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1843\u001b[0m                             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1844\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1845\u001b[0;31m                             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "steps = len(train_data)/batch_size\n",
    "steps_val = len(val_data)/batch_size\n",
    "print steps\n",
    "print steps_val\n",
    "\n",
    "\n",
    "csv_logger = CSVLogger('run4_adam.csv')\n",
    "lr_plateau = ReduceLROnPlateau(monitor='val_loss', patience=1, verbose=1, factor=0.5)\n",
    "checkpoint = ModelCheckpoint(filepath='/home/pieter/projects/MLIP-Brainforest/bloomingtest/OWN/model.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "                             verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit_generator(train_gen, steps_per_epoch=steps,\n",
    "                    epochs=50, verbose=1,\n",
    "                    callbacks=[csv_logger, lr_plateau, checkpoint],\n",
    "                    validation_data=val_gen, validation_steps=steps_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 MLIP Keras Theano",
   "language": "python",
   "name": "mlippython2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
